<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="description" content=""> <meta name="keywords" content=""> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>AirLetters</title> <link rel="icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-DHVEPP2GQR"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DHVEPP2GQR");</script> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="../assets/css/bulma.min.css"> <link rel="stylesheet" href="../assets/css/bulma-carousel.min.css"> <link rel="stylesheet" href="../assets/css/bulma-slider.min.css"> <link rel="stylesheet" href="../assets/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="../assets/css/nerfies_general.css"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer src="../assets/js/fontawesome.all.min.js"></script> <script src="../assets/js/bulma-carousel.min.js"></script> <script src="../assets/js/bulma-slider.min.js"></script> <script src="../assets/js/nerfies_index.js"></script> <script src="../assets/js/nerfies_comparision.js"></script> <script>function toggleCodeBlock(){var e=document.getElementById("bashCodeBlock");"none"===e.style.display?e.style.display="block":e.style.display="none"}</script> <style>h1 img{width:50px;height:auto;vertical-align:middle}table{width:100%;border-collapse:collapse;margin:20px 0;font-family:system-ui,-apple-system,sans-serif}th,td{padding:12px;text-align:left}th{background-color:#f5f5f5;font-weight:600;border-bottom:2px solid #ddd}tr:hover{background-color:#f0f0f0}.section-header{background-color:#e0e0e0;font-weight:bold;border-bottom:1px solid #ddd;border-top:1px solid #ddd}.subsection{padding-left:24px}</style> </head> <body> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title"> <span>‚úâÔ∏è</span>AirLetters<span>üí®:</span> An Open Video Dataset of Characters Drawn in the Air</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://www.cs.toronto.edu/~rishit">Rishit Dagli</a><sup>1,2</sup>, <a href="https://scholar.google.ca/citations?user=OY4_O9UAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Guillaume Berger</a><sup>1</sup>, <a href="https://joaanna.github.io/" rel="external nofollow noopener" target="_blank">Joanna Materzynska</a><sup>3</sup>, <a href="https://scholar.google.com/citations?user=eXWO8nwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ingo Bax</a><sup>1</sup>, <a href="http://www.iro.umontreal.ca/~memisevr/" rel="external nofollow noopener" target="_blank">Roland Memisevic</a><sup>1</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> <sup>1</sup> Qualcomm AI Research <sup>*</sup>, <sup>2</sup> University of Toronto, <sup>3</sup> MIT<br> <sup>*</sup> Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. </span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="/~rishit/projects/airletters/paper.pdf" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="https://arxiv.org/abs/2410.02921" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span> <span class="link-block"> <a href="" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code (coming soon)</span> </a> </span> <span class="link-block"> <a href="https://www.qualcomm.com/developer/software/airletters-dataset" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="fas fa-database"></i> </span> <span>Data</span> </a> </span> <span class="link-block"> <a href="https://huggingface.co/spaces/rishitdagli/airletters-leaderboard" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="fas fa-trophy"></i> </span> <span>Leaderboard</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="content"> <div class="container" style="text-align: center;"> <h6 class="subtitle has-text-centered"> Note: This is an unofficial project page that I made after I finished my internship. </h6> </div> <section class="hero"> <div class="container" style="text-align: center;"> <h4 class="subtitle has-text-centered"> <span class="dnerf">AirLetters</span> is a new video dataset consisting of real-world videos of human-generated, articulated motions. </h4> <video poster="" id="steve" autoplay="" muted="" loop="" playsinline="" style="margin: 0 auto; display: block; max-width: 100%; height: auto;"> <source src="../assets/img/airletters/con.mp4" type="video/mp4"></source> </video> </div> </section> <p><br></p> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2>Abstract</h2> <div class="content has-text-justified"> We introduce AirLetters, a new video dataset consisting of real-world videos of human-generated, articulated motions. Specifically, our dataset requires a vision model to predict letters that humans draw in the air. Unlike existing video datasets, accurate classification predictions for AirLetters rely critically on discerning motion patterns and on integrating long-range information in the video over time. An extensive evaluation of state-of-the-art image and video understanding models on AirLetters shows that these methods perform poorly and fall far behind a human baseline. Our work shows that, despite recent progress in end-to-end video understanding, accurate representations of complex articulated motions ‚Äì a task that is trivial for humans ‚Äì remains an open problem for end-to-end learning. </div> </div> </div> <hr> <h2 id="motivation">Motivation</h2> <ul> <li>We highlight a significant gap in current end-to-end video understanding and activity recognition methods: all models, especially large vision language models, perform well below human evaluation results. Human evaluation achieves near-perfect accuracy, while the task is challenging for all tested models. Unlike existing video datasets, accurate video understanding on our dataset requires detailed understanding of motion in the video and the integration of long-range temporal information across the entire video.</li> <li>This dataset requires models to attend through the entire video to perform well, and increasing the number of frames that models attend to significantly increases their performance.</li> <li>We show that classes such as the digits ‚Äú0‚Äù, ‚Äú1‚Äù, and ‚Äú2‚Äù are particularly challenging, as they are easily confused with each other. In contrast, the contrast classes ‚ÄúDoing Nothing‚Äù and ‚ÄúDoing Other Things‚Äù, are more easily recognized which demonstrates the challenging nature of understanding precise motions for video understanding models.</li> </ul> <h2 id="contents">Dataset Contents</h2> <p>We focus on manual articulations of each letter of the Latin alphabet as well as numeric digits. This amounts to 36 primary gesture classes, for which recognition requires temporal and spatial analysis of the video. The dataset also includes two contrast classes designed to refine the sensitivity and specificity of recognition systems trained on our dataset. The ‚ÄúDoing Nothing‚Äù class includes videos of individuals in non-active states, such as sitting or standing still, to represent periods of inactivity within human-computer interactions, and the ‚ÄúDoing Other Things‚Äù class consists of clips capturing miscellaneous, non-communicative movements such as adjusting position or random hand movements.</p> <div class="content has-text-justified"> <img src="../assets/img/airletters/air-letter-crowd-workers.jpg" style="max-width: 70%; margin: 0 auto;"> <p>Figure: <b>Diversity in our Dataset.</b> Each of the images is taken from a randomly sampled video from our dataset. Our dataset has a large variance in the appearance of subjects, background, occlusion, and lighting conditions in the videos.</p> </div> <p>Our dataset has videos with precise articulated motion across many different frames instead of many other dataset which only need 1 frame or 2-4 frames to understand the video.</p> <div class="content has-text-justified"> <img src="../assets/img/airletters/variation.png" style="max-width: 70%; margin: 0 auto;"> <p>Figure: <b>Challenges due to inter-class similarities and intra-class diversity.</b> We show some examples of drawing the letter ‚ÄúB‚Äù and the digit of ‚Äú3‚Äù, where differentiating both of these classes also requires understanding depth and velocity of relative motion to understand if the individual intended to draw a vertical line (for ‚ÄúB‚Äù) or only meant to place their hands in position (for ‚Äú3‚Äù). Underneath, we show examples of variability in drawing the letter ‚ÄúY‚Äù. For example, in one way version of drawing the letter ‚ÄúY‚Äù, only the last few frames show a stroke that distinguishes it from the letter ‚ÄúX‚Äù.</p> </div> <p>Choosing a few key frames in our dataset does not help in understanding the video, and the entire video needs to be considered to understand the motion.</p> <div class="content has-text-justified"> <img src="../assets/img/airletters/scaling.png" style="max-width: 70%; margin: 0 auto;"> <p>Figure: <b>Scaling Training Frames.</b> Performance of models across different numbers of training frames. The Pareto Frontier is represented by a black curve. Note that this dataset requires models to attend through the entire video to perform well, and increasing the number of frames that models attend to significantly increases their performance.</p> </div> <h2 id="stats">Dataset Statistics</h2> <table> <thead> <tr> <th>Statistic</th> <th>Value</th> </tr> </thead> <tbody> <tr class="section-header"> <td colspan="2">Total Statistics</td> </tr> <tr> <td class="subsection">Videos</td> <td>161,652</td> </tr> <tr> <td class="subsection" style="padding-left: 36px;">Training Split</td> <td>128,745</td> </tr> <tr> <td class="subsection" style="padding-left: 36px;">Validation Split</td> <td>16,480</td> </tr> <tr> <td class="subsection" style="padding-left: 36px;">Test Split</td> <td>16,427</td> </tr> <tr> <td class="subsection">Classes</td> <td>38</td> </tr> <tr> <td class="subsection">Actors</td> <td>1,781</td> </tr> <tr> <td class="subsection">Frames</td> <td>40,142,100</td> </tr> <tr class="section-header"> <td colspan="2">Median Statistics (with Standard Deviation)</td> </tr> <tr> <td class="subsection">Duration</td> <td>2.93 (¬±0.13)</td> </tr> <tr> <td class="subsection">FPS</td> <td>30.0 (¬±0.0)</td> </tr> <tr> <td class="subsection">Videos per Class (√ó10¬≥)</td> <td>4.04 (¬±1.31)</td> </tr> <tr> <td class="subsection">Videos per Actor</td> <td>40.0 (¬±99.29)</td> </tr> </tbody> </table> <h2 id="citation">Citation</h2> <div style="position: relative;"> <pre><code id="citationCode">
@inproceedings{dagliairletters,
    title={AirLetters: An Open Video Dataset of Characters Drawn in the Air},
    author={Dagli, Rishit and Berger, Guillaume and Materzynska, Joanna and Bax, Ingo and Memisevic, Roland},
    booktitle={European Conference on Computer Vision Workshops},
    year={2024}
}</code></pre> <button class="button is-small is-dark" onclick="copyCitation()" style="position: absolute; top: 10px; right: 10px;"> <span class="icon"> <i class="fas fa-copy"></i> </span> </button> </div> <script>function copyCitation(){var t=document.getElementById("citationCode").innerText;navigator.clipboard.writeText(t)}</script> </div> </div> </section> <footer class="footer"> <div class="container"> <div class="content has-text-centered"> <a class="icon-link" href="blank"> <i class="fas fa-file-pdf"></i> </a> <a class="icon-link" href="https://github.com/Rishit-dagli" disabled rel="external nofollow noopener" target="_blank"> <i class="fab fa-github"></i> </a> </div> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io" rel="external nofollow noopener" target="_blank">NeRFies</a>. </p> <p> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p> </div> </div> </div> </div> </footer> </body> </html>