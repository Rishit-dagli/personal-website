<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="description" content=""> <meta name="keywords" content=""> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>QIVD</title> <link rel="icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-DHVEPP2GQR"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DHVEPP2GQR");</script> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="../assets/css/bulma.min.css"> <link rel="stylesheet" href="../assets/css/bulma-carousel.min.css"> <link rel="stylesheet" href="../assets/css/bulma-slider.min.css"> <link rel="stylesheet" href="../assets/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="../assets/css/nerfies_general.css"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer src="../assets/js/fontawesome.all.min.js"></script> <script src="../assets/js/bulma-carousel.min.js"></script> <script src="../assets/js/bulma-slider.min.js"></script> <script src="../assets/js/nerfies_index.js"></script> <script src="../assets/js/nerfies_comparision.js"></script> <link rel="stylesheet" href="https://fonts.cdnfonts.com/css/itc-avant-garde-gothic-std-book"> <style>.publication-title,.publication-authors,.author-block{font-family:'ITC Avant Garde Gothic Std','Avant Garde',sans-serif!important;font-weight:500!important}</style> <script>function toggleCodeBlock(){var e=document.getElementById("bashCodeBlock");"none"===e.style.display?e.style.display="block":e.style.display="none"}</script> <style>h1 img{width:50px;height:auto;vertical-align:middle}table{width:100%;border-collapse:collapse;margin:20px 0;font-family:system-ui,-apple-system,sans-serif}th,td{padding:12px;text-align:left}th{background-color:#f5f5f5;font-weight:600;border-bottom:2px solid #ddd}tr:hover{background-color:#f0f0f0}.section-header{background-color:#e0e0e0;font-weight:bold;border-bottom:1px solid #ddd;border-top:1px solid #ddd}.subsection{padding-left:24px}</style> </head> <body> <nav class="navbar" role="navigation" aria-label="main navigation"> <div class="navbar-brand"> <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"> <span aria-hidden="true"></span> </a> </div> <div class="navbar-menu"> <div class="navbar-start" style="flex-grow: 1; justify-content: center;"> <a class="navbar-item" href="../"> <span class="icon"> <i class="fas fa-home"></i> </span> </a> <div class="navbar-item has-dropdown is-hoverable"> <a class="navbar-link"> More Research </a> <div class="navbar-dropdown"> <a class="navbar-item" href="/airletters"> AirLetters </a> <a class="navbar-item" href="https://arxiv.org/abs/2407.08101" rel="external nofollow noopener" target="_blank"> QEVD </a> <a class="navbar-item" href="https://arxiv.org/abs/2411.09052" rel="external nofollow noopener" target="_blank"> ClevrSkills </a> </div> </div> </div> </div> </nav> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title">Can Vision-Language Models Answer Face to Face Questions in the Real-World?</h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://scholar.google.ca/citations?user=BqNYf-oAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Reza Pourreza</a><sup>*1</sup>, <a href="/">Rishit Dagli</a><sup>*†2</sup>, <a href="https://apratimbhattacharyya18.github.io/" rel="external nofollow noopener" target="_blank">Apratim Bhattacharyya</a><sup>1</sup>, <a href="https://sunnypanchal.ca/" rel="external nofollow noopener" target="_blank">Sunny Panchal</a><sup>1</sup>, <a href="https://scholar.google.ca/citations?user=OY4_O9UAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Guillaume Berger</a><sup>1</sup>, <a href="http://www.iro.umontreal.ca/~memisevr/" rel="external nofollow noopener" target="_blank">Roland Memisevic</a><sup>1</sup> </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"> <sup>1</sup> Qualcomm AI Research <sup>‡</sup>, <sup>2</sup> University of Toronto<br> <sup>*</sup> Equal Contribution. <sup>†</sup> Work completed during internship at Qualcomm AI Research.<br> </span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="/~rishit/projects/qivd/paper.pdf" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="https://arxiv.org/abs/2503.19356" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span> <span class="link-block"> <a href="" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code (coming soon)</span> </a> </span> <span class="link-block"> <a href="" class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fas fa-database"></i> </span> <span>Data (coming soon)</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="section"> <div class="container is-max-desktop"> <div class="content"> <style>@import url('https://fonts.cdnfonts.com/css/itc-avant-garde-gothic-std-book');body{position:relative}body::before{content:"";background-image:url('../assets/img/ghibli.jpg');background-size:cover;background-position:center;background-attachment:fixed;position:fixed;top:0;left:0;width:100%;height:100%;opacity:.3;z-index:-1}.footer{background-color:transparent!important;position:relative;z-index:1}.footer .container{background-color:rgba(255,255,255,0.5);padding:20px;border-radius:8px}.qivd-page *{font-family:'ITC Avant Garde Gothic Std','Avant Garde',sans-serif!important;font-weight:500!important}.qivd-page h2{text-align:center;margin-top:2rem;margin-bottom:1.5rem}.qivd-page img{display:block;margin-left:auto;margin-right:auto;max-width:100%;height:auto;margin-top:1.5rem;margin-bottom:1.5rem}.author-list{display:flex;flex-wrap:wrap;justify-content:space-around;gap:1.5%;margin:1.5rem auto;max-width:100%}.author-item{display:flex;flex-direction:column;align-items:center;text-align:center;width:15%;min-width:100px;margin-bottom:1rem}.author-img{width:80px;height:80px;border-radius:50%;object-fit:cover;margin-bottom:8px;border:2px solid rgba(0,0,0,0.1)}.easter-egg{margin-top:30px;cursor:pointer;color:#3273dc}.typing-effect{overflow:hidden;white-space:normal;margin:0 auto;letter-spacing:.1em;margin-top:30px;cursor:pointer;max-width:80%;line-height:1.5}.results-carousel{overflow:hidden}.results-carousel .item{margin:0 auto;max-width:800px;background-color:rgba(255,255,255,0.7);border-radius:8px;padding:10px;display:flex;align-items:center;justify-content:center;height:350px}.results-carousel video{display:block;height:300px;width:auto;max-width:100%;object-fit:contain}.hero.is-light{background-color:rgba(255,255,255,0.7)!important}</style> <section class="hero"> <div class="container" style="text-align: center;"> <h4>We present <span class="dnerf">QIVD</span> to assess situated AI, where AI systems must interpret and respond to real-time visual and audio inputs or talk talk to a user in the real world and understand what's happening right now </h4> <img src="../assets/img/qivd/teaser.png" alt="QIVD" style="width: 100%; height: auto;"> </div> </section> <section class="hero is-light is-small"> <div class="hero-body"> <div class="container"> <div id="qivd-results-carousel" class="carousel results-carousel"> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000003.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000007.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000017.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000028.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000154.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000367.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000392.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000465.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000624.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000641.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00000801.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001000.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001171.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001202.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001221.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001286.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001433.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001534.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001622.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001735.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001788.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00001887.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002099.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002149.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002303.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002389.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002411.mp4" type="video/mp4"></source> </video> </div> <div class="item"> <video poster="" autoplay="" controls="" muted="" loop="" playsinline=""> <source src="../assets/img/qivd/videos/00002464.mp4" type="video/mp4"></source> </video> </div> </div> </div> </div> </section> <p><br></p> <div class="qivd-page"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2>Abstract</h2> <div class="content has-text-justified"> AI models have made significant strides in recent years in their ability to describe and answer questions about realworld images. They have also made progress in the ability to converse with users in real-time using audio input. This raises the question: have we reached the point where AI models, connected to a camera and microphone, can converse with users in real-time about scenes and events that are unfolding live in front of the camera? This has been a long-standing goal in AI and is a prerequisite for real-world AI assistants and humanoid robots to interact with humans in everyday situations. In this work, we introduce a new dataset and benchmark, the Qualcomm Interactive Video Dataset (IVD), which allows us to assess the extent to which existing models can support these abilities, and to what degree these capabilities can be instilled through fine-tuning. The dataset is based on a simple question-answering setup, where users ask questions that the system has to answer, in real-time, based on the camera and audio input. We show that existing models fall behind human performance on this task, and we identify the main sources for the performance gap. However, we also show that for many of the required perceptual skills, fine-tuning on this form of data can significantly reduce this gap. </div> </div> </div> <h2 id="QIVD"> <span class="dnerf">QIVD</span> Benchmark</h2> We present <span class="dnerf">QIVD</span>, a novel benchmark created to assess the non-expert multimodal situated understanding capability of foundation models. Our benchmark for each instance includes a video (with audio) of a user asking a question about the video, the transcribed question, the ground truth answer, and a ground-truth timestamp at which the question should be answered.<br><br> All the videos in the benchamrk have been crowdsourced and then annotated by non-expert annotators. Following this the videos go through a rigororus quality check process and semantic categorization process. <br><br> We use a situated question-answering setup, where we have a streaming video, and the model must decide if the question has been asked and if there is enough information to answer the question. If so the models must answer the question. Particularly, answering the questions do not require any domain expertise. <video src="../assets/img/qivd/teaser.mp4" autoplay="" loop="" muted="" playsinline=""></video> <h2 id="citation">Experiment Results</h2> <span class="dnerf">QIVD</span> does not require any domainspecific knowledge or complex reasoning skills. Yet we show that the task is still highly challenging for LMMs. Particularly, models fall far behind human performance on sitiated understanding.<br><br> All of the models have been tested in:<br><br> 1. streaming setup: models must decide if the question has been asked and if there is enough information to answer the question. If so the models must answer the question. More details in the paper.<br> 2. offline setup: models must answer the question based on the entire video and the question.<br><br> We experimented with both open-source and closed-source multimodal-models, finetuned models on the benchmark, and allowed models to process audio. We evaluate the performance of all the models in a zero-shot setting and conduct prompt engineering if models do not provide any prompts for such tasks. <h3>Leaderboard</h3> <h4>Streaming Setup</h4> <div class="table-container"> <table id="leaderboard-table" class="table is-striped is-hoverable is-fullwidth"> <thead> <tr class="has-background-info-light"> <th onclick="sortTable(0)">Model <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortTable(1)">Corr. ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortTable(2)">BERT ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortTable(3)">METEOR ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortTable(4)">BLEU ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortTable(5)">ROUGE-L ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> </tr> </thead> <tbody> <tr> <td>Chat-UniVi</td> <td>39.69</td> <td>89.94</td> <td>37.47</td> <td>6.08</td> <td>28.45</td> </tr> <tr> <td>InstructBLIP</td> <td>37.17</td> <td>82.19</td> <td>4.35</td> <td>0.02</td> <td>10.00</td> </tr> <tr> <td>LLaMA-VID</td> <td>43.48</td> <td>90.51</td> <td>37.19</td> <td>5.84</td> <td>29.80</td> </tr> <tr> <td>LLaVA-NeXT</td> <td>24.97</td> <td>85.29</td> <td>22.85</td> <td>1.38</td> <td>11.64</td> </tr> <tr> <td>Video-ChatGPT</td> <td>35.38</td> <td>90.53</td> <td>38.13</td> <td>7.58</td> <td>31.08</td> </tr> <tr> <td>VideoChat</td> <td>8.00</td> <td>85.05</td> <td>23.48</td> <td>1.08</td> <td>12.22</td> </tr> <tr> <td>VideoChat2</td> <td>46.07</td> <td><u>91.13</u></td> <td><u>45.49</u></td> <td>11.35</td> <td><u>41.38</u></td> </tr> <tr> <td>Video-LLaVA</td> <td>23.52</td> <td>87.77</td> <td>27.15</td> <td>1.98</td> <td>19.31</td> </tr> <tr> <td>VideoLLaMA</td> <td>33.52</td> <td>89.50</td> <td>39.06</td> <td>7.62</td> <td>30.84</td> </tr> <tr> <td>VideoLLaMA2-7B</td> <td>44.31</td> <td>91.18</td> <td><b>47.20</b></td> <td><u>13.93</u></td> <td>40.63</td> </tr> <tr> <td>VideoLLaMA2-72B</td> <td>47.69</td> <td><b>91.42</b></td> <td>46.58</td> <td><b>14.03</b></td> <td><b>41.70</b></td> </tr> <tr> <td>VideoLLaMA3-7B</td> <td><u>52.31</u></td> <td>90.92</td> <td>45.20</td> <td>11.21</td> <td>40.54</td> </tr> <tr> <td>Qwen2.5-VL-7B</td> <td><b>53.55</b></td> <td>87.17</td> <td>34.95</td> <td>3.88</td> <td>26.52</td> </tr> </tbody> </table> </div> <p class="has-text-centered mt-2">Overall results of different models on the <span class="dnerf">QIVD</span> leaderboard. The <b style="color: #209cee;">best-performing model</b> in each category is in-bold, and the <u style="color: #3273dc;">second best</u> is underlined. Corr. represents the correctness score calculated by the LLM judge.</p> <script>function sortTable(e){var a,r,s,n,t,o,i,l,f=0;for(a=document.getElementById("leaderboard-table"),s=!0,l="asc";s;){for(s=!1,r=a.rows,n=1;n<r.length-1;n++)if(i=!1,t=r[n].getElementsByTagName("TD")[e],o=r[n+1].getElementsByTagName("TD")[e],"asc"==l){if(0==e){if(t.innerHTML.toLowerCase()>o.innerHTML.toLowerCase()){i=!0;break}}else if(parseFloat(t.innerText)>parseFloat(o.innerText)){i=!0;break}}else if("desc"==l)if(0==e){if(t.innerHTML.toLowerCase()<o.innerHTML.toLowerCase()){i=!0;break}}else if(parseFloat(t.innerText)<parseFloat(o.innerText)){i=!0;break}i?(r[n].parentNode.insertBefore(r[n+1],r[n]),s=!0,f++):0==f&&"asc"==l&&(l="desc",s=!0)}var T=a.querySelectorAll("thead th span.icon");for(n=0;n<T.length;n++)T[n].innerHTML='<i class="fas fa-sort"></i>';var c=r[0].getElementsByTagName("TH")[e].querySelector("span.icon");c.innerHTML="desc"==l?'<i class="fas fa-sort-down"></i>':'<i class="fas fa-sort-up"></i>'}</script> <style>#leaderboard-table th,#offline-leaderboard-table th{cursor:pointer;user-select:none}#leaderboard-table th:hover,#offline-leaderboard-table th:hover{background-color:rgba(0,0,0,0.05)}#leaderboard-table b,#offline-leaderboard-table b{font-weight:bold;color:#209cee}#leaderboard-table u,#offline-leaderboard-table u{text-decoration:underline;color:#3273dc}.table-container{background-color:rgba(255,255,255,0.7);border-radius:8px;padding:1rem;margin-bottom:1.5rem}.table{border:1px solid rgba(0,0,0,0.1)}.table th,.table td{border:1px solid rgba(0,0,0,0.1)}</style> <h4>Offline Setup</h4> <div class="table-container"> <table id="offline-leaderboard-table" class="table is-striped is-hoverable is-fullwidth"> <thead> <tr class="has-background-info-light"> <th onclick="sortOfflineTable(0)">Model <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortOfflineTable(1)">Corr. ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortOfflineTable(2)">BERT ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortOfflineTable(3)">METEOR ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortOfflineTable(4)">BLEU ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> <th onclick="sortOfflineTable(5)">ROUGE-L ↑ <span class="icon"><i class="fas fa-sort"></i></span> </th> </tr> </thead> <tbody> <tr> <td>Chat-UniVi</td> <td>45.10</td> <td>90.50</td> <td>40.02</td> <td>7.24</td> <td>31.22</td> </tr> <tr> <td>InstructBLIP</td> <td>41.14</td> <td>82.03</td> <td>4.54</td> <td>0.07</td> <td>10.72</td> </tr> <tr> <td>LLaMA-VID</td> <td>48.48</td> <td>90.78</td> <td>37.55</td> <td>5.42</td> <td>29.82</td> </tr> <tr> <td>LLaVA-NeXT</td> <td>28.90</td> <td>85.78</td> <td>24.50</td> <td>1.67</td> <td>13.22</td> </tr> <tr> <td>Video-ChatGPT</td> <td>40.76</td> <td>91.01</td> <td>40.59</td> <td>9.07</td> <td>33.58</td> </tr> <tr> <td>VideoChat</td> <td>8.31</td> <td>85.20</td> <td>24.39</td> <td>1.03</td> <td>12.54</td> </tr> <tr> <td>VideoChat2</td> <td>53.07</td> <td>91.52</td> <td>47.93</td> <td>12.43</td> <td><u>43.87</u></td> </tr> <tr> <td>Video-LLaVA</td> <td>18.62</td> <td>83.38</td> <td>2.90</td> <td>0.00</td> <td>15.66</td> </tr> <tr> <td>VideoLLaMA</td> <td>39.21</td> <td>90.45</td> <td>43.88</td> <td>9.86</td> <td>34.93</td> </tr> <tr> <td>VideoLLaMA2-7B</td> <td>52.69</td> <td><u>91.71</u></td> <td><u>51.08</u></td> <td><b>16.41</b></td> <td>43.84</td> </tr> <tr> <td>VideoLLaMA2-72B</td> <td>53.41</td> <td><b>92.29</b></td> <td><u>51.13</u></td> <td><u>16.12</u></td> <td><b>45.76</b></td> </tr> <tr> <td>VideoLLaMA3-7B</td> <td>59.62</td> <td>91.63</td> <td>48.56</td> <td>12.72</td> <td>43.84</td> </tr> <tr> <td>Qwen2.5-VL-7B</td> <td><u>60.00</u></td> <td>87.58</td> <td>37.37</td> <td>4.66</td> <td>29.44</td> </tr> <tr> <td>GPT-4o</td> <td><b>66.38</b></td> <td>89.36</td> <td><b>51.18</b></td> <td>15.72</td> <td>42.55</td> </tr> <tr class="has-background-info-light"> <td>Human (subset)</td> <td>89.00</td> <td>93.01</td> <td>53.21</td> <td>17.40</td> <td>49.76</td> </tr> </tbody> </table> </div> <p class="has-text-centered mt-2">Overall results of different models on the <span class="dnerf">QIVD</span> leaderboard. The <b style="color: #209cee;">best-performing model</b> in each category is in-bold, and the <u style="color: #3273dc;">second best</u> is underlined. Corr. represents the correctness score calculated by the LLM judge.</p> <script>function sortOfflineTable(e){var a,r,n,s,t,o,i,l,f=0;for(a=document.getElementById("offline-leaderboard-table"),n=!0,l="asc";n;){for(n=!1,r=a.rows,s=1;s<r.length-1;s++)if(i=!1,t=r[s].getElementsByTagName("TD")[e],o=r[s+1].getElementsByTagName("TD")[e],"asc"==l){if(0==e){if(t.innerHTML.toLowerCase()>o.innerHTML.toLowerCase()){i=!0;break}}else if(parseFloat(t.innerText)>parseFloat(o.innerText)){i=!0;break}}else if("desc"==l)if(0==e){if(t.innerHTML.toLowerCase()<o.innerHTML.toLowerCase()){i=!0;break}}else if(parseFloat(t.innerText)<parseFloat(o.innerText)){i=!0;break}i?(r[s].parentNode.insertBefore(r[s+1],r[s]),n=!0,f++):0==f&&"asc"==l&&(l="desc",n=!0)}var T=a.querySelectorAll("thead th span.icon");for(s=0;s<T.length;s++)T[s].innerHTML='<i class="fas fa-sort"></i>';var c=r[0].getElementsByTagName("TH")[e].querySelector("span.icon");c.innerHTML="desc"==l?'<i class="fas fa-sort-down"></i>':'<i class="fas fa-sort-up"></i>'}</script> <h3>Error Analysis</h3> Analysis of model errors reveals that vision-language models struggle most with temporal understanding, fine-grained visual details, and spatial relationships within scenes. The performance gap between the best AI models (66.38% for GPT-4o) and humans (89.00%) highlights significant room for improvement in situated understanding. Common failure modes include hallucinating non-existent objects, focusing on irrelevant visual details, and failing to track objects across video frames. Models consistently perform worse in streaming setups compared to offline evaluation, indicating challenges in real-time decision-making. These findings suggest that while vision-language models have made impressive progress, significant challenges remain in achieving human-like performance in real-world interactions. <img src="../assets/img/qivd/eval.png" alt="QIVD Leaderboard" style="width: 100%; height: auto;"> <img src="../assets/img/qivd/radar.png" alt="QIVD Leaderboard" style="width: 50%; height: auto;"> <h3>Qualitative Results</h3> We showcase some qualitative results from various models on the QIVD benchmark. The examples illustrate common error types including object hallucination, misinterpreting spatial relationships, ignoring temporal context, and producing inconsistent answers in complex environments. These qualitative samples complement the quantitative metrics by providing insight into specific model limitations in real-world scenarios. <img src="../assets/img/qivd/failure.png" alt="QIVD Qualitative Results" style="width: 100%; height: auto;"> <h2 id="citation">Citation</h2> <div style="position: relative;"> <pre><code id="citationCode">
@misc{pourreza2025visionlanguagemodelsanswerface,
      title={Can Vision-Language Models Answer Face to Face Questions in the Real-World?}, 
      author={Reza Pourreza and Rishit Dagli and Apratim Bhattacharyya and Sunny Panchal and Guillaume Berger and Roland Memisevic},
      year={2025},
      eprint={2503.19356},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.19356}, 
}</code></pre> <button class="button is-small is-dark" onclick="copyCitation()" style="position: absolute; top: 10px; right: 10px;"> <span class="icon"> <i class="fas fa-copy"></i> </span> </button> </div> <div id="easter-egg" class="easter-egg" onclick="toggleEasterEgg()">Easter Egg</div> </div> <script>function copyCitation(){var e=document.getElementById("citationCode").innerText;navigator.clipboard.writeText(e)}function toggleEasterEgg(){var e=document.getElementById("easter-egg");if(eggRevealed)e.classList.remove("typing-effect"),e.classList.add("easter-egg"),e.innerHTML="Easter Egg",eggRevealed=!1;else{e.classList.remove("easter-egg"),e.classList.add("typing-effect"),e.innerHTML="";var t=0,a=25;function g(){t<message.length?(e.innerHTML+=message.charAt(t),t++,setTimeout(g,a)):(typing=!1,eggRevealed=!0)}typing=!0,g()}}var eggRevealed=!1,typing=!1,message="this project was released on march 26, 2025 when the internet was taken over with ghibli style images and thus this ghibli-style page.";</script> </div> </div> </section> <footer class="footer"> <div class="container"> <div class="content has-text-centered"> <a class="icon-link" href="blank"> <i class="fas fa-file-pdf"></i> </a> <a class="icon-link" href="https://github.com/Rishit-dagli" disabled rel="external nofollow noopener" target="_blank"> <i class="fab fa-github"></i> </a> </div> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io" rel="external nofollow noopener" target="_blank">NeRFies</a>. </p> <p> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p> </div> </div> </div> </div> </footer> </body> </html>